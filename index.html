
<p><b>Agenda for the Data Centric Engineering Reading Group</b></p>
<p><b>Alan Turing Institute, UK </b></p>
<p>For more information about the reading group, or to join the mailing list, contact Marina Riabiz, FX Briol or Chris Oates.</p>
<p>For more information about the Data Centric Engineering programme, visit <u><a href="https://www.turing.ac.uk/research/research-programmes/data-centric-engineering">https://www.turing.ac.uk/research/research-programmes/data-centric-engineering</a></u></p>



<p>=============================================================</p>
<p>Optimization</p>
<p>=============================================================</p>

<p><b>06/02/19: Stochastic Gradient Descent (Marina Riabiz)</b></p>
<ul class="small"><li>Leon Bottou, Frank E. Curtis, Jorge Nocedal, <i>Optimization Methods for Large-Scale Machine Learning,</i> <u><a href="https://arxiv.org/pdf/1606.04838.pdf#page21">https://arxiv.org/pdf/1606.04838.pdf#page21</a></u></li></ul></ul>

<p><b>13/02/19: Proof of convergence rate of Stochastic Gradient Descent (</b><b>Ömer Deniz Akyıldız</b><b>)</b></p>

<p><b>20/02/19: Proof of convergence rate of Gradient Descent (Ömer Deniz Akyıldız)</b></p>
<ul class="small"><li>Robert M. Gower<i>, Convergence Theorems for Gradient Descent, </i><u><a href="https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf">https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf</a></u></li></ul>
<ul class="small"><li>Ji Liu, Stochastic Gradient ‘Descent’ Algorithm  <u>https://www.cs.rochester.edu/u/jliu/CSC-576/class-note-10.pdf</u>, </li></ul>

<p><b>27/02/19: Stochastic Gradient Langevin Dynamics</b><i> </i><b>(Andrew Duncan)</b></p>
<ul class="small"><li>Maxim Raginsky, Alexander Rakhlin, Matus Telgarsky, <i>Non-Convex Learning via Stochastic Gradient Langevin Dynamics: A Nonasymptotic Analysis,</i> <u>https://arxiv.org/pdf/1702.03849.pdf</u></li></ul></ul>

<p><b>06/03/2019</b><b>: Conjugate Gradient Methods (Taha Ceriti)</b></p>
<ul class="small"><li>Chris Bishop, <i>Neural Networks for Pattern Recognition</i>, Chapter 7. </li></ul>
<ul class="small"><li>J.R. Shewchuk, <i>An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</i>, 1994 <u>https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</u></li></ul>


<p>Extra References:</p>

<ul class="small"><li>S. Bubeck, Convex Optimization: Algorithms and Complexity. In Foundations and Trends in Machine Learning, Vol. 8: No. 3-4, pp 231-357, 2015. <u><a href="http://sbubeck.com/Bubeck15.pdf" rel="nofollow">http://sbubeck.com/Bubeck15.pdf</a></u></li></ul>
<ul class="small"><li>Nagapetyan et al., The True Cost of SGLD, <u>https://arxiv.org/pdf/1706.02692.pdf</u></li></ul>
<ul class="small"><li>Brosse at al, The promises and pitfalls of Stochastic Gradient Langevin Dynamics, <u>https://arxiv.org/pdf/1811.10072.pdf</u></li></ul>
<ul class="small"><li>Dalalyan and Karagulyan, User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient, <u>https://arxiv.org/pdf/1710.00095.pdf</u></li></ul>
<ul class="small"><li>Vollmer et al., Exploration of the (Non-)asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics, <u>https://arxiv.org/pdf/1501.00438.pdf</u></li></ul>





<p>=============================================================</p>
<p>Gaussian Processes and RKHS</p>
<p>=============================================================</p>

<p><b>13/03/19: Hyperparameter estimation for Gaussian Processes (Alex Diaz)</b> </p>
<ul class="small"><li>Rassmussen, C.E., Gaussian Processes for Machine Learning (Ch2 and Ch 5) <u><a href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf" rel="nofollow">http://www.gaussianprocess.org/gpml/chapters/RW.pdf</a></u></li></ul>
<ul class="small"><li>DiazDelaO, F.A. et al. (2017) Bayesian updating and model class selection with Subset Simulation <u> https://www.sciencedirect.com/science/article/pii/S0045782516308283</u></li></ul>

<ul class="small"><li>Garbuno-Inigo, A. et al. (2016) Gaussian process hyper-parameter estimation using Parallel Asymptotically Independent Markov Sampling <u>https://www.sciencedirect.com/science/article/pii/S0167947316301311</u></li></ul></ul>

<ul class="small"><li>Garbuno-Inigo, A. et al. (2016) Transitional annealed adaptive slice sampling for Gaussian process hyper-parameter estimation
<u><a http://www.dl.begellhouse.com/journals/52034eb04b657aea,55c0c92f02169163,2f3449c01b48b322.html" rel="nofollow">http://www.dl.begellhouse.com/journals/52034eb04b657aea,55c0c92f02169163,2f3449c01b48b322.html</a></u></li></ul></ul>


<p><b>20/03/19: Gaussian Interpolation/Regression Error Bounds (George Wynne)</b> </p>
<ul class="small"><li>Holger Wendland, Christien Rieger, Approximate Interpolation with Applications to Selecting Smoothing Parameters <u>https://link.springer.com/article/10.1007/s00211-005-0637-y</u></li></ul></ul>


<p><b>27/03/19: Structure of the Gaussian RKHS (Toni Karvonen)</b></p>
<ul class="small"><li>Ha Quang Minh, <i>Some Properties of Gaussian Reproducing Kernel Hilbert Spaces and Their Implications for Function Approximation and Learning Theory, </i><u>https://link.springer.com/article/10.1007/s00365-009-9080-0</u></li></ul></ul>


<p>Extra References:</p>
<p>Steinwart, I., Hush, D., & Scovel, C. (2006). An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels. IEEE Transactions on Information Theory, 52(10), 4635–4643</p>




<p>=============================================================</p>
<p>Invited Talks + Deep GPs</p>
<p>=============================================================</p>

<p><b>03/04/19: Bayesian synthetic likelihood (Leah South)</b></p>
<ul class="small"><li>L. F. Price, C. C. Drovandi, A. Lee & D. J. Nott, <i>Bayesian Synthetic Likelihood, </i><u>https://www.tandfonline.com/doi/abs/10.1080/10618600.2017.1302882</u> </li></ul></ul>

<p><b>12/04/19: Deep Gaussian Processes (Kangrui Wang) </b></p>
<ul class="small"><li>Damianou A, Lawrence N. Deep Gaussian processes <u><a href="http://proceedings.mlr.press/v31/damianou13a.pdf" rel="nofollow">http://proceedings.mlr.press/v31/damianou13a.pdf</a></u></li></ul></ul>

<ul class="s§mall"><li>Dunlop M M, Girolami M A, Stuart A M, et al. How deep are deep Gaussian processes?
	<u><a href="http://www.jmlr.org/papers/volume19/18-015/18-015.pdf" rel="nofollow">http://www.jmlr.org/papers/volume19/18-015/18-015.pdf</a></u></li></ul></ul>
<ul class="small"><li>Bauer M, van der Wilk M, Rasmussen C E. Understanding probabilistic sparse Gaussian process approximations
</i><u>https://arxiv.org/abs/1606.04820</u> </li></ul></ul>

<p><b>17/04/19: </b><b>Multi Level Monte Carlo (</b><b>Alastair Gregory</b><b>)</b></p>
<ul class="small"><li><a href="http://people.maths.ox.ac.uk/gilesm/files/cgst.pdf" rel="nofollow">http://people.maths.ox.ac.uk/gilesm/files/cgst.pdf</a></li></ul>
<ul class="small"><li>https://people.maths.ox.ac.uk/gilesm/files/OPRE_2008.pdf</li></ul></ul>



<p><b>24/04/19: Adaptive Bayesian Quadrature (Matthew Fisher)</b></p>



<p>=============================================================</p>
<p>MMD & Stein’s method</p>
<p>=============================================================</p>

<p><b>01/05/19: Controlling Convergence with Maximum Mean Discrepancy (Chris Oates)</b></p>
<ul class="small"><li>Gretton, A., Borgwardt, K., Rasch, M., Schölkopf, B. and Smola, A.J., 2007. <i>A kernel method for the two-sample-problem</i>. <u><a href="http://papers.nips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf" rel="nofollow">http://papers.nips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf</a></u> </li></ul>

<p><b>08/05/19: Introduction to Stein’s method (FX Briol)</b></p>
<ul class="small"><li>Talk will cover Chapter 2 of “Chen, L. H. Y., Goldstein, L., & Shao, Q.-M. (2011). Normal Approximation by Stein’s Method. Springer.”</li></ul></ul>

<ul class="small"><li>Gorham,  J., Duncan, A., Mackey, L., & Vollmer, S. (2016). Measuring Sample Quality with Diffusions. ArXiv:1506.03039.</li></ul></ul>
<ul class="small"><li>Gorham, J., & Mackey, L. (2017). Measuring Sample Quality with Kernels. In Proceedings of the International Conference on Machine Learning (pp. 1292–1301).</li></ul></ul>

<p><b>15/05/19: </b><b>Bochner's Theorem and Maximum Mean Discrepancy</b><b> (George Wynne)</b></p>
<ul class="small"><li>Theorem 9 in <u><a href="http://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf" rel="nofollow">http://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf</a></u> is the result of Bochner's theorem being used in MMD</li></ul></ul>

<ul class="small"><li><u><a href="http://www.math.nus.edu.sg/~matsr/ProbI/Lecture7.pdf" rel="nofollow">http://www.math.nus.edu.sg/~matsr/ProbI/Lecture7.pdf</a></u><b> </b>is a source for proof of Bochner's theorem</li></ul></ul>


<p>Extra References:</p>
<p>https://sites.google.com/site/steinsmethod/home</p>


<p>=============================================================</p>
<p>Greedy Algorithms</p>
<p>=============================================================</p>

<p><b>19/06/19: Convergence Guarantees for Adaptive Bayesian Quadrature Methods (Motonobu Kanagawa)</b></p>
<ul class="small"><li>Kanagawa, M. and Hennig, P, 2019. <i>Convergence Guarantees for Adaptive Bayesian Quadrature Methods</i>. <u>https://arxiv.org/abs/1905.10271</u> </li></ul>



<p>=============================================================</p>
<p>Uncertainty Exploration in Aerospace </p>
<p>=============================================================</p>

<p><b>26/06/19: </b><b>Using machine learning to predict and understand turbulence modelling uncertainties </b><b>(Ashley Scillitoe)</b></p>
	
<p><b>03/07/19: Polynomial approximations in uncertainty quantification (Pranay Seshadri)</b></p>
<ul class="small"><li>Talk will cover Xiu D (2010) “Numerical methods for stochastic computations: a spectral method approach”. Princeton University Press.</li></ul></ul>
<ul class="small"><li>In David Blackwell Room at 11</li></ul>

<p>=============================================================</p>
<p>Bayesian Learning and Algorithms </p>
<p>=============================================================</p>
	
<p><b>10/07/19: A Kernel Stein Test for Comparing Latent Variable Models (Heishiro Kanagawa)</b></p>	
<ul class="small"><li>Kanagawa, H., Jitkrittum, W., Mackey, L., Fukumizu, K., Gretton, A. (2019) <i>A Kernel Stein Test for Comparing Latent Variable Models. </i> <u>https://arxiv.org/abs/1907.00586</u> </li></ul>	
<ul class="small"><li>In Mary Shelly Room at 11</li></ul>	
	
<p><b>17/07/19: Multi-resolution Multi-task Gaussian Processes (Oliver Hamelijnck)</b></p>	
<ul class="small"><li>Hamelijnck, O., Damoulas, T., Wang, K., Girolami, M. (2019) <i>Multi-resolution Multi-task Gaussian Processes. </i> <u>https://arxiv.org/pdf/1906.08344.pdf</u> </li></ul>	
<ul class="small"><li>In Mary Shelly Room at 11</li></ul>
	
<p><b>24/07/19: A Primer on PAC Bayesian Learning (Benjamin Guedj)</b></p>	
<ul class="small"><li>Talk will cover <a href="https://bguedj.github.io/icml2019/index.html">this tutorial from ICML 2019</a></li></ul>
<ul class="small"><li>In Mary Shelly Room at 11</li></ul>
	
<p>=============================================================</p>
<p>Gradient Flows </p>
<p>=============================================================</p>

<p><b>31/07/19: An Introduction to Measure Transport (Chris Oates)</b></p>
<ul class="small"><li>In Mary Shelly Room at 11</li></ul>
	
<p><b>06/08/19: Gradient Flows for Statistical Computation (Marina Riabiz)</b></p>
<ul class="small"><li>In Mary Shelly Room at 11</li></ul>
<ul class="small"><li> Daneri and Savare', Lecture Notes on Gradient Flows and Optimal Transport, <u>https://arxiv.org/abs/1009.3737</u></li></ul>
<ul class="small"><li> N. Garcia Trillos, D. Sanz-Alonso, The Bayesian update: variational formulations and gradient flows, <u>https://arxiv.org/abs/1705.07382</u></li></ul>
<ul class="small"><li> G.Peyre' and M. Cuturi, Computational Optimal Transport, Chapter 9.3 <u>https://arxiv.org/pdf/1803.00567.pdf</u></li></ul>
<ul class="small"><li>  Carrillo, Craig, Patacchini, A blob method for diffusion, <u>https://arxiv.org/abs/1709.09195</u></li></ul>
<ul class="small"><li> Sides <u>  http://web.math.ucsb.edu/~kcraig/math/curriculum_vitae_files/NIPS_120917.pdf</u></li></ul>

	
<p><b>14/08/19: The Mathematics of Gradient Flows (Andrew Duncan)</b></p>
<ul class="small"><li>In Mary Shelly Room at 11</li></ul>	
	
<p><b>14/08/19: Displacement Convexity and Implications for Variational Inference (Andrew Duncan)</b></p>
<ul class="small"><li>In David Blackwell Room at 1</li></ul>	
	
<p>=============================================================</p>
<p> Estimators for Intractable Models </p>
<p>=============================================================</p>

<p><b>23/08/19: Comparing spatial models in the presence of spatial smoothing (Earl Duncan)</b></p>
<ul class="small"><li>In Mary Shelly Room at 11</li></ul>	
	
<p><b>28/08/19: Fisher efficient inference of intractable models (Song Liu)</b></p>
<ul class="small"><li>Liu, S., Kanamori, T., Jitkrittum, W., & Chen, Y. (2018). <i>Fisher efficient inference of intractable models. </i> <u>https://arxiv.org/abs/1805.07454</u> </li></ul>
<ul class="small"><li>In Mary Shelly Room at 11</li></ul>
	
<p><b>04/09/19: Statistical Inference for Generative Models with Maximum Mean Discrepancy (FX Briol)</b></p>
<ul class="small"><li>Briol, F-X, Barp, A., Duncan, A. B., Girolami, M. (2019) <i>Statistical inference for generative models with maximum mean discrepancy. </i> <u>https://arxiv.org/abs/1906.05944</u> </li></ul>
<ul class="small"><li>In Mary Shelly Room at 11</li></ul>
	
<p>=============================================================</p>
<p>Uncertainty Quantification and Probabilistic Numerics </p>
<p>=============================================================</p>	
	
<p><b>11/09/19: A New Approach to Probabilistic Rounding Error Analysis (Jon Cockayne)</b></p>
<ul class="small"><li>Talk will cover Higham, N., Mary, T. (2018) <i>A new approach to probabilistic rounding analysis. </i> <u>http://eprints.maths.manchester.ac.uk/2673/1/paper.pdf</u> </li></ul>	
<ul class="small"><li>In Mary Shelly Room at 11</li></ul>	
	
<p><b>25/09/19: Model Inference for Ordinary Differential Equations by Parametric Polynomial Kernel Regression (David Green)</b></p>
<ul class="small"><li>In Lovelace Room at 11</li></ul>	
	
<p><b>02/10/19: The Ridgelet Transform and a Quadrature of Neural Networks (Takuo Matsubara)</b></p>
<ul class="small"><li>In David Blackwell Room at 11</li></ul>
	
<p><b>09/10/19: Title TBC (Jarno Vanhatalo)</b></p>
<ul class="small"><li>In Mary Shelly Room at 11</li></ul>

	
<p>=============================================================</p>
<p> PAC Bayes </p>
<p>=============================================================</p>
	
<p><b>16/10/19: TBC (Omar Rivasplata)</b></p>
<ul class="small"><li>Location: TBC </li></ul>	
