
<p><b> Alan Turing Institute - Data Centric Engineering Reading Group 2019 </b></p>


<p>=============================================================</p>
<p>Optimization</p>
<p>=============================================================</p>

<p><b>06/02/19: Stochastic Gradient Descent (Marina Riabiz)</b></p>
<ul class="small"><li>Leon Bottou, Frank E. Curtis, Jorge Nocedal, <i>Optimization Methods for Large-Scale Machine Learning,</i> <u>https://arxiv.org/pdf/1606.04838.pdf#page21</u></li></ul></ul>

<p><b>13/02/19: Proof of convergence rate of Stochastic Gradient Descent (</b><b>Ömer Deniz Akyıldız</b><b>)</b></p>

<p><b>20/02/19: Proof of convergence rate of Gradient Descent (Ömer Deniz Akyıldız)</b></p>
<ul class="small"><li>Robert M. Gower<i>, Convergence Theorems for Gradient Descent, </i><u>https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf</u></li></ul>
<ul class="small"><li>Ji Liu, Stochastic Gradient ‘Descent’ Algorithm  <u>https://www.cs.rochester.edu/u/jliu/CSC-576/class-note-10.pdf</u>, </li></ul>

<p><b>27/02/19: Stochastic Gradient Langevin Dynamics</b><i> </i><b>(Andrew Duncan)</b></p>
<ul class="small"><li>Maxim Raginsky, Alexander Rakhlin, Matus Telgarsky, <i>Non-Convex Learning via Stochastic Gradient Langevin Dynamics: A Nonasymptotic Analysis,</i> <u>https://arxiv.org/pdf/1702.03849.pdf</u></li></ul></ul>

<p><b>06/03/2019</b><b>: Conjugate Gradient Methods (Taha Ceriti)</b></p>
<ul class="small"><li>Chris Bishop, <i>Neural Networks for Pattern Recognition</i>, Chapter 7. </li></ul>
<ul class="small"><li>J.R. Shewchuk, <i>An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</i>, 1994 <u>https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</u></li></ul>


<p>Extra References:</p>

<ul class="small"><li>S. Bubeck, Convex Optimization: Algorithms and Complexity. In Foundations and Trends in Machine Learning, Vol. 8: No. 3-4, pp 231-357, 2015. <u><a href="http://sbubeck.com/Bubeck15.pdf" rel="nofollow">http://sbubeck.com/Bubeck15.pdf</a></u></li></ul>
<ul class="small"><li>Nagapetyan et al., The True Cost of SGLD, <u>https://arxiv.org/pdf/1706.02692.pdf</u></li></ul>
<ul class="small"><li>Brosse at al, The promises and pitfalls of Stochastic Gradient Langevin Dynamics, <u>https://arxiv.org/pdf/1811.10072.pdf</u></li></ul>
<ul class="small"><li>Dalalyan and Karagulyan, User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient, <u>https://arxiv.org/pdf/1710.00095.pdf</u></li></ul>
<ul class="small"><li>Vollmer et al., Exploration of the (Non-)asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics, <u>https://arxiv.org/pdf/1501.00438.pdf</u></li></ul>

<p>=============================================================</p>
<p>Gaussian Processes and RKHS</p>
<p>=============================================================</p>

<p><b>13/03/19: Hyperparameter estimation for Gaussian Processes (Alex Diaz)</b> </p>
<ul class="small"><li>Rassmussen, C.E., Gaussian Processes for Machine Learning (Ch2 and Ch 5) <u> href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf" rel="nofollow">http://www.gaussianprocess.org/gpml/chapters/RW.pdf</u></li></ul>
<ul class="small"><li>DiazDelaO, F.A. et al. (2017) Bayesian updating and model class selection with Subset Simulation
<p>https://www.sciencedirect.com/science/article/pii/S0045782516308283</p>
<ul class="small"><li>Garbuno-Inigo, A. et al. (2016) Gaussian process hyper-parameter estimation using Parallel Asymptotically Independent Markov Sampling</li></ul></ul>
<p>https://www.sciencedirect.com/science/article/pii/S0167947316301311</p>
<ul class="small"><li>Garbuno-Inigo, A. et al. (2016) Transitional annealed adaptive slice sampling for Gaussian process hyper-parameter estimation
<p><a href="http://www.dl.begellhouse.com/journals/52034eb04b657aea,55c0c92f02169163,2f3449c01b48b322.html" rel="nofollow">http://www.dl.begellhouse.com/journals/52034eb04b657aea,55c0c92f02169163,2f3449c01b48b322.html</a></p>

<p><b>20/03/19</b><b>: </b><b>Gaussian Interpolation/Regression Error Bounds </b><b>(George Wynne)</b></p>
<ul class="small"><li>Holger Wendland, Christien Rieger, Approximate Interpolation with Applications to Selecting Smoothing Parameters</li></ul></ul>
<p>https://link.springer.com/article/10.1007/s00211-005-0637-y</p>

<p><b>27/03/19: Structure of the Gaussian RKHS (Toni Karvonen)</b></p>
<ul class="small"><li>Ha Quang Minh, <i>Some Properties of Gaussian Reproducing Kernel Hilbert Spaces and Their Implications for Function Approximation and Learning Theory, </i><u>https://link.springer.com/article/10.1007/s00365-009-9080-0</u></li></ul></ul>


<p>Extra References:</p>
<p>Steinwart, I., Hush, D., & Scovel, C. (2006). An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels. IEEE Transactions on Information Theory, 52(10), 4635–4643</p>

<p>=============================================================</p>
<p>Invited Talks + Deep GPs</p>
<p>=============================================================</p>

<p><b>03/04/19: Bayesian synthetic likelihood (Leah South)</b></p>
<ul class="small"><li>L. F. Price, C. C. Drovandi, A. Lee & D. J. Nott, <i>Bayesian Synthetic Likelihood, </i><u>https://www.tandfonline.com/doi/abs/10.1080/10618600.2017.1302882</u> </li></ul></ul>

<p><b>12/04/19: Deep Gaussian Processes (Kangrui Wang) </b></p>
<ul class="small"><li>Damianou A, Lawrence N. Deep Gaussian processes</li></ul></ul>
<p><a href="http://proceedings.mlr.press/v31/damianou13a.pdf" rel="nofollow">http://proceedings.mlr.press/v31/damianou13a.pdf</a></p>
<ul class="s§mall"><li>Dunlop M M, Girolami M A, Stuart A M, et al. How deep are deep Gaussian processes?</li></ul></ul>
<p><a href="http://www.jmlr.org/papers/volume19/18-015/18-015.pdf" rel="nofollow">http://www.jmlr.org/papers/volume19/18-015/18-015.pdf</a></p>
<ul class="small"><li>Bauer M, van der Wilk M, Rasmussen C E. Understanding probabilistic sparse Gaussian process approximations
</i><u>https://arxiv.org/abs/1606.04820</u> </li></ul></ul>

<p><b>17/04/19: </b><b>Multi Level Monte Carlo (</b><b>Alastair Gregory</b><b>)</b></p>
<ul class="small"><li><a href="http://people.maths.ox.ac.uk/gilesm/files/cgst.pdf" rel="nofollow">http://people.maths.ox.ac.uk/gilesm/files/cgst.pdf</a></li></ul>
<ul class="small"><li>https://people.maths.ox.ac.uk/gilesm/files/OPRE_2008.pdf</li></ul></ul>



<p><b>24/04/19: Adaptive Bayesian Quadrature (Matthew Fisher)</b></p>





















<p>=============================================================</p>
<p>MMD & Stein’s method</p>
<p>=============================================================</p>

<p><b>01/05/19: Controlling Convergence with Maximum Mean Discrepancy (Chris Oates)</b></p>
<li>Gretton, A., Borgwardt, K., Rasch, M., Schölkopf, B. and Smola, A.J., 2007. <i>A kernel method for the two-sample-problem</i>. <u><a href="http://papers.nips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf" rel="nofollow">http://papers.nips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf</a></u> </li></ul>

<p><b>08/05/19: Introduction to Stein’s method (FX Briol)</b></p>
<ul class="small"><li>Talk will cover Chapter 2 of “Chen, L. H. Y., Goldstein, L., & Shao, Q.-M. (2011). Normal Approximation by Stein’s Method. Springer.”</li></ul>
<li>Gorham,  J., Duncan, A., Mackey, L., & Vollmer, S. (2016). Measuring Sample </li></ul>
<p>Quality with Diffusions. ArXiv:1506.03039.</p>
<ul class="small"><li>Gorham, J., & Mackey, L. (2017). Measuring Sample Quality with Kernels. In Proceedings of the International Conference on Machine Learning (pp. 1292–1301).</li></ul></ul>

<p><b>15/05/19: </b><b>Bochner's Theorem and Maximum Mean Discrepancy</b><b> (George Wynne)</b></p>
<ul class="small"><li>Theorem 9 in <b><u><a href="http://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf" rel="nofollow">http://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf</a></b></u><b> </b>is the result of Bochner's theorem being used in MMD</li></ul></ul>

<ul class="small"><li><b><u><a href="http://www.math.nus.edu.sg/~matsr/ProbI/Lecture7.pdf" rel="nofollow">http://www.math.nus.edu.sg/~matsr/ProbI/Lecture7.pdf</a></b></u><b> </b>is a source for proof of Bochner's theorem</li></ul></ul>


<p>Extra References:</p>
<p>https://sites.google.com/site/steinsmethod/home</p>


<p>=============================================================</p>
<p>Greedy Algorithms</p>
<p>=============================================================</p>

<p><b>19/06/19: Convergence Guarantees for Adaptive Bayesian Quadrature Methods (Motonobu Kanagawa)</b></p>
<li>Kanagawa, M. and Hennig, P, 2019. <i>Convergence Guarantees for Adaptive Bayesian Quadrature Methods</i>. <u>https://arxiv.org/abs/1905.10271</u> </li></ul>





<p></p>
<p>=============================================================</p>
<p>Uncertainty exploration in aerospace (working title!)</p>
<p>=============================================================</p>

<p><b>27/06/19: Polynomial approximations in uncertainty quantification (Pranay Seshadri)</b></p>
<ul class="small"><li><b>The Design of Resilient Engineering Infrastructure Systems (Jonathan Mak)</b></li></ul></ul>

<li>Talk will cover Xiu D (2010) “Numerical methods for stochastic computations: a spectral method approach”. Princeton University Press.</li></ul>

<p><b>03/07/19: </b><b>Using machine learning to predict and understand turbulence modelling uncertainties </b><b>(Ashley Scillitoe)</b></p>
<li>TBC</li></ul>
<p><b>	</b></p>


